{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faceswap-GAN_lite_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MiOo3478pJg9"
      },
      "source": [
        "# Before we start...\n",
        "\n",
        "This colab notebook is a minimum demo for faceswap-GAN v2.2. Since colab allows maximum run time limit of 12 hrs, we will only train a lightweight model in this notebook. **The purpose of this notebook is not to train a model that produces high quality results but a quick overview for how faceswap-GAN works.**\n",
        "\n",
        "The pipeline of faceswap-GAN v2.2 is described below:\n",
        "\n",
        "  1. Upload two videos for training.\n",
        "  2. Apply face extraction (preprocessing) on the two uploaded videos\n",
        "  3. Train a liteweight faceswap-GAN model. (This will take 10 ~ 12 hrs)\n",
        "  4. Apply video conversion to the uploaded videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qf8Wvh5Wrm3V"
      },
      "source": [
        "# Step 1: Set runtime type to Python 3/GPU\n",
        "Set the colab notebook to GPU instance through: **runtime -> change runtime type -> Python3 and GPU**\n",
        "\n",
        "The following cells will show the system information of the current instance. Run the cells and check if it uses python >= 3.6 and has a GPU device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1zQhhPfKrmQl",
        "outputId": "3d0a4da9-301e-449e-b333-628a482567e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import platform\n",
        "print(platform.python_version())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1f96C1TApC00",
        "outputId": "84dc2f8a-d3c6-470d-ac68-58dea52e6163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 11356599744026834758, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 12527219961800391070\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 132360369610286955\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14892338381\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 221088165982976795\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XOsTN_gjzonZ"
      },
      "source": [
        "# Step 2: Git clone faceswap-GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L4VioT5YpJQB",
        "outputId": "7c8b168f-2c0d-42e7-a866-73c1ecf2911a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!git clone https://github.com/evanxephon/faceswap-GAN.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'faceswap-GAN'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects:  10% (1/10)   \u001b[K\rremote: Counting objects:  20% (2/10)   \u001b[K\rremote: Counting objects:  30% (3/10)   \u001b[K\rremote: Counting objects:  40% (4/10)   \u001b[K\rremote: Counting objects:  50% (5/10)   \u001b[K\rremote: Counting objects:  60% (6/10)   \u001b[K\rremote: Counting objects:  70% (7/10)   \u001b[K\rremote: Counting objects:  80% (8/10)   \u001b[K\rremote: Counting objects:  90% (9/10)   \u001b[K\rremote: Counting objects: 100% (10/10)   \u001b[K\rremote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 1023 (delta 3), reused 0 (delta 0), pack-reused 1013\u001b[K\n",
            "Receiving objects: 100% (1023/1023), 2.21 MiB | 4.80 MiB/s, done.\n",
            "Resolving deltas: 100% (625/625), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_4C8o3y0DFD",
        "outputId": "e34e10b1-9700-4c10-fa31-21a2c6145dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd \"faceswap-GAN\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/faceswap-GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K0wsds0OslRc"
      },
      "source": [
        "# Step 3: Upload training videos\n",
        "\n",
        "The user should upload two videos: **source video** and **target video**. The model will **tranform source face to target face by default.**\n",
        "\n",
        "  - The videos better **contain only one person**.\n",
        "  - There is no limitation on video length but the longer it is, the longer preprocessing time / video conversion time it will take, which may cause excceded run time of 12 hrs. (**Recommended video length: 30 secs ~ 2 mins.**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOM8TGEzbUhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "bd6d4413-17b4-4de1-db41-4140c3852a03"
      },
      "source": [
        "!git clone https://github.com/1adrianb/face-alignment.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'face-alignment'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects:  20% (1/5)   \u001b[K\rremote: Counting objects:  40% (2/5)   \u001b[K\rremote: Counting objects:  60% (3/5)   \u001b[K\rremote: Counting objects:  80% (4/5)   \u001b[K\rremote: Counting objects: 100% (5/5)   \u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects:  20% (1/5)   \u001b[K\rremote: Compressing objects:  40% (2/5)   \u001b[K\rremote: Compressing objects:  60% (3/5)   \u001b[K\rremote: Compressing objects:  80% (4/5)   \u001b[K\rremote: Compressing objects: 100% (5/5)   \u001b[K\rremote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "Receiving objects:   0% (1/549)   \rReceiving objects:   1% (6/549)   \rReceiving objects:   2% (11/549)   \rReceiving objects:   3% (17/549)   \rReceiving objects:   4% (22/549)   \rReceiving objects:   5% (28/549)   \rReceiving objects:   6% (33/549)   \rReceiving objects:   7% (39/549)   \rReceiving objects:   8% (44/549)   \rReceiving objects:   9% (50/549)   \rReceiving objects:  10% (55/549)   \rReceiving objects:  11% (61/549)   \rReceiving objects:  12% (66/549)   \rReceiving objects:  13% (72/549)   \rReceiving objects:  14% (77/549)   \rReceiving objects:  15% (83/549)   \rReceiving objects:  16% (88/549)   \rReceiving objects:  17% (94/549)   \rReceiving objects:  18% (99/549)   \rReceiving objects:  19% (105/549)   \rReceiving objects:  20% (110/549)   \rReceiving objects:  21% (116/549)   \rReceiving objects:  22% (121/549)   \rReceiving objects:  23% (127/549)   \rReceiving objects:  24% (132/549)   \rReceiving objects:  25% (138/549)   \rReceiving objects:  26% (143/549)   \rReceiving objects:  27% (149/549)   \rReceiving objects:  28% (154/549)   \rReceiving objects:  29% (160/549)   \rReceiving objects:  30% (165/549)   \rReceiving objects:  31% (171/549)   \rReceiving objects:  32% (176/549)   \rReceiving objects:  33% (182/549)   \rReceiving objects:  34% (187/549)   \rReceiving objects:  35% (193/549)   \rReceiving objects:  36% (198/549)   \rReceiving objects:  37% (204/549)   \rReceiving objects:  38% (209/549)   \rReceiving objects:  39% (215/549)   \rReceiving objects:  40% (220/549)   \rReceiving objects:  41% (226/549)   \rremote: Total 549 (delta 0), reused 0 (delta 0), pack-reused 544\u001b[K\n",
            "Receiving objects:  42% (231/549)   \rReceiving objects:  43% (237/549)   \rReceiving objects:  44% (242/549)   \rReceiving objects:  45% (248/549)   \rReceiving objects:  46% (253/549)   \rReceiving objects:  47% (259/549)   \rReceiving objects:  48% (264/549)   \rReceiving objects:  49% (270/549)   \rReceiving objects:  50% (275/549)   \rReceiving objects:  51% (280/549)   \rReceiving objects:  52% (286/549)   \rReceiving objects:  53% (291/549)   \rReceiving objects:  54% (297/549)   \rReceiving objects:  55% (302/549)   \rReceiving objects:  56% (308/549)   \rReceiving objects:  57% (313/549)   \rReceiving objects:  58% (319/549)   \rReceiving objects:  59% (324/549)   \rReceiving objects:  60% (330/549)   \rReceiving objects:  61% (335/549)   \rReceiving objects:  62% (341/549)   \rReceiving objects:  63% (346/549)   \rReceiving objects:  64% (352/549)   \rReceiving objects:  65% (357/549)   \rReceiving objects:  66% (363/549)   \rReceiving objects:  67% (368/549)   \rReceiving objects:  68% (374/549)   \rReceiving objects:  69% (379/549)   \rReceiving objects:  70% (385/549)   \rReceiving objects:  71% (390/549)   \rReceiving objects:  72% (396/549)   \rReceiving objects:  73% (401/549)   \rReceiving objects:  74% (407/549)   \rReceiving objects:  75% (412/549)   \rReceiving objects:  76% (418/549)   \rReceiving objects:  77% (423/549)   \rReceiving objects:  78% (429/549)   \rReceiving objects:  79% (434/549)   \rReceiving objects:  80% (440/549)   \rReceiving objects:  81% (445/549)   \rReceiving objects:  82% (451/549)   \rReceiving objects:  83% (456/549)   \rReceiving objects:  84% (462/549)   \rReceiving objects:  85% (467/549)   \rReceiving objects:  86% (473/549)   \rReceiving objects:  87% (478/549)   \rReceiving objects:  88% (484/549)   \rReceiving objects:  89% (489/549)   \rReceiving objects:  90% (495/549)   \rReceiving objects:  91% (500/549)   \rReceiving objects:  92% (506/549)   \rReceiving objects:  93% (511/549)   \rReceiving objects:  94% (517/549)   \rReceiving objects:  95% (522/549)   \rReceiving objects:  96% (528/549)   \rReceiving objects:  97% (533/549)   \rReceiving objects:  98% (539/549)   \rReceiving objects:  99% (544/549)   \rReceiving objects: 100% (549/549)   \rReceiving objects: 100% (549/549), 3.51 MiB | 19.87 MiB/s, done.\n",
            "Resolving deltas:   0% (0/330)   \rResolving deltas:   6% (21/330)   \rResolving deltas:   7% (26/330)   \rResolving deltas:   8% (28/330)   \rResolving deltas:   9% (32/330)   \rResolving deltas:  11% (38/330)   \rResolving deltas:  12% (41/330)   \rResolving deltas:  13% (43/330)   \rResolving deltas:  14% (47/330)   \rResolving deltas:  15% (50/330)   \rResolving deltas:  17% (59/330)   \rResolving deltas:  19% (63/330)   \rResolving deltas:  22% (75/330)   \rResolving deltas:  24% (80/330)   \rResolving deltas:  36% (119/330)   \rResolving deltas:  45% (150/330)   \rResolving deltas:  46% (154/330)   \rResolving deltas:  47% (156/330)   \rResolving deltas:  48% (159/330)   \rResolving deltas:  60% (201/330)   \rResolving deltas:  61% (202/330)   \rResolving deltas:  67% (222/330)   \rResolving deltas:  68% (225/330)   \rResolving deltas:  69% (228/330)   \rResolving deltas:  70% (232/330)   \rResolving deltas:  71% (236/330)   \rResolving deltas:  76% (252/330)   \rResolving deltas:  93% (308/330)   \rResolving deltas:  98% (325/330)   \rResolving deltas:  99% (327/330)   \rResolving deltas: 100% (330/330)   \rResolving deltas: 100% (330/330), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRO-nONob_2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('./face-alignment/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd9umRDbcg8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ./faceA\n",
        "!mkdir ./faceA/rgb\n",
        "!mkdir ./faceB\n",
        "!mkdir ./faceB/rgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBltX51qcYNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv ../cage.zip ./faceA/rgb\n",
        "%cd './faceA/rgb'\n",
        "!unzip ./cage.zip\n",
        "!rm ./cage.zip\n",
        "%cd '../../'\n",
        "\n",
        "!mv ../trump.zip ./faceB/rgb\n",
        "%cd './faceB/rgb'\n",
        "!unzip ./trump.zip\n",
        "!rm ./trump.zip\n",
        "%cd '../../'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGzbKn3mfC91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm ./faceB/rgb/trump.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWV0nmqpbl7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import face_alignment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2UigQHYdWin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from pathlib import PurePath, Path\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFmfwI3xd1zf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir_faceA = \"./faceA/rgb\"\n",
        "dir_faceB = \"./faceB/rgb\"\n",
        "dir_bm_faceA_eyes = \"./binary_masks/faceA_eyes\"\n",
        "dir_bm_faceB_eyes = \"./binary_masks/faceB_eyes\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNB09jo1d40Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fns_faceA = glob(f\"{dir_faceA}/*.*\")\n",
        "fns_faceB = glob(f\"{dir_faceB}/*.*\")\n",
        "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)\n",
        "# !mkdir -p binary_masks/faceA_eyes\n",
        "Path(f\"binary_masks/faceA_eyes\").mkdir(parents=True, exist_ok=True)\n",
        "# !mkdir -p binary_masks/faceB_eyes\n",
        "Path(f\"binary_masks/faceB_eyes\").mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fo4Qy2EeMRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fns_face_not_detected = []\n",
        "\n",
        "for idx, fns in enumerate([fns_faceA, fns_faceB]):\n",
        "    if idx == 0:\n",
        "        save_path = dir_bm_faceA_eyes\n",
        "    elif idx == 1:\n",
        "        save_path = dir_bm_faceB_eyes     \n",
        "    \n",
        "    # create binary mask for each training image\n",
        "    for fn in fns:\n",
        "        raw_fn = PurePath(fn).parts[-1]\n",
        "\n",
        "        x = plt.imread(fn)\n",
        "        x = cv2.resize(x, (256,256))\n",
        "        preds = fa.get_landmarks(x)\n",
        "        \n",
        "        if preds is not None:\n",
        "            preds = preds[0]\n",
        "            mask = np.zeros_like(x)\n",
        "            \n",
        "            # Draw right eye binary mask\n",
        "            pnts_right = [(preds[i,0],preds[i,1]) for i in range(36,42)]\n",
        "            hull = cv2.convexHull(np.array(pnts_right)).astype(np.int32)\n",
        "            mask = cv2.drawContours(mask,[hull],0,(255,255,255),-1)\n",
        "\n",
        "            # Draw left eye binary mask\n",
        "            pnts_left = [(preds[i,0],preds[i,1]) for i in range(42,48)]\n",
        "            hull = cv2.convexHull(np.array(pnts_left)).astype(np.int32)\n",
        "            mask = cv2.drawContours(mask,[hull],0,(255,255,255),-1)\n",
        "\n",
        "            # Draw mouth binary mask\n",
        "            #pnts_mouth = [(preds[i,0],preds[i,1]) for i in range(48,60)]\n",
        "            #hull = cv2.convexHull(np.array(pnts_mouth)).astype(np.int32)\n",
        "            #mask = cv2.drawContours(mask,[hull],0,(255,255,255),-1)\n",
        "            \n",
        "            mask = cv2.dilate(mask, np.ones((13,13), np.uint8), iterations=1)\n",
        "            mask = cv2.GaussianBlur(mask, (7,7), 0)\n",
        "            \n",
        "        else:\n",
        "            mask = np.zeros_like(x)\n",
        "            print(f\"No faces were detected in image '{fn}''\")\n",
        "            fns_face_not_detected.append(fn)\n",
        "        \n",
        "        plt.imsave(fname=f\"{save_path}/{raw_fn}\", arr=mask, format=\"jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxL66E7Gecly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "078cb814-7f92-4055-c4bc-40fb3f6d5cc2"
      },
      "source": [
        "num_faceA = len(glob(dir_faceA+\"/*.*\"))\n",
        "num_faceB = len(glob(dir_faceB+\"/*.*\"))\n",
        "\n",
        "print(\"Nuber of processed images: \"+ str(num_faceA + num_faceB))\n",
        "print(\"Number of image(s) with no face detected: \" + str(len(fns_face_not_detected)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nuber of processed images: 694\n",
            "Number of image(s) with no face detected: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EQ0HH6VUpJHW",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "thlzp88qpJJk",
        "colab": {}
      },
      "source": [
        "# Upload source video\n",
        "source_video = files.upload()\n",
        "\n",
        "for fn_source_video, _ in source_video.items():\n",
        "    print(fn_source_video)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YyKu2hcCpJKu",
        "colab": {}
      },
      "source": [
        "# Upload target video\n",
        "target_video = files.upload()\n",
        "\n",
        "for fn_target_video, _ in target_video.items():\n",
        "    print(fn_target_video)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFlAV6Umaztz",
        "colab_type": "text"
      },
      "source": [
        "# Step 4: Set maximum training iterations\n",
        "Default 25000 iters require ~ 10hrs of training.\n",
        "\n",
        "Iterations >= 27k may exceed run time limit; Iterations < 18k may yield poorly-trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkIEuPuCazt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global TOTAL_ITERS\n",
        "TOTAL_ITERS =30000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PS25Uu9kxDwo"
      },
      "source": [
        "# Step 5: Everything is ready.\n",
        "\n",
        "**Press Ctrl + F10 (or runtime -> run after)** to start the remaining process and leave this page alone. It will take 10 ~ 12 hours to finish training. The result video can be downloaded by running the last cell: \n",
        "  ```python\n",
        "  files.download(\"OUTPUT_VIDEO.mp4\")\n",
        "  # Some browsers do not support this line (e.g., Opera does not pop up a save dialog). Please use Firefox or Chrome.\n",
        "  ```\n",
        "Notice that **this page should not be closed or refreshed while running**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BY3qysVq0p2P",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install moviepy\n",
        "!pip install keras_vggface\n",
        "import imageio\n",
        "imageio.plugins.ffmpeg.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ym0EsJk9pJRw",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "from detector.face_detector import MTCNNFaceDetector\n",
        "import glob\n",
        "\n",
        "from preprocess import preprocess_video"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "otuzS3Di0gvz",
        "colab": {}
      },
      "source": [
        "fd = MTCNNFaceDetector(sess=K.get_session(), model_path=\"./mtcnn_weights/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kt5FVEt11B2K",
        "colab": {}
      },
      "source": [
        "!mkdir -p faceA/rgb\n",
        "!mkdir -p faceA/binary_mask\n",
        "!mkdir -p faceB/rgb\n",
        "!mkdir -p faceB/binary_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dO11aRsZ0gyK",
        "colab": {}
      },
      "source": [
        "save_interval = 5 # perform face detection every {save_interval} frames\n",
        "save_path = \"./faceA/\"\n",
        "preprocess_video(fn_source_video, fd, save_interval, save_path)\n",
        "save_path = \"./faceB/\"\n",
        "preprocess_video(fn_target_video, fd, save_interval, save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qIb9TSMz0g0g",
        "colab": {}
      },
      "source": [
        "print(str(len(glob.glob(\"faceA/rgb/*.*\"))) + \" face(s) extracted from source video: \" + fn_source_video + \".\")\n",
        "print(str(len(glob.glob(\"faceB/rgb/*.*\"))) + \" face(s) extracted from target video: \" + fn_target_video + \".\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O9L5UW8U3AaQ"
      },
      "source": [
        "## The following cells are from [FaceSwap_GAN_v2.2_train_test.ipynb](https://github.com/shaoanlu/faceswap-GAN/blob/master/FaceSwap_GAN_v2.2_train_test.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WtyeXpEc2lDO"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MtENXluJ0g2I",
        "outputId": "5a28a31a-7379-4d91-e181-995ab72e2112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.layers import *\n",
        "import keras.backend as K\n",
        "import tensorflow as tf"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFAokeU12Vff",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import PurePath, Path\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EJ1miHVk2ns7"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BjrPsIbZ2ViU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "203cd15f-ee38-4b3d-ab79-33aa053733cc"
      },
      "source": [
        "K.set_learning_phase(1)\n",
        "# Number of CPU cores\n",
        "num_cpus = os.cpu_count()\n",
        "\n",
        "# Input/Output resolution\n",
        "RESOLUTION = 64 # 64x64, 128x128, 256x256\n",
        "assert (RESOLUTION % 64) == 0, \"RESOLUTION should be 64, 128, or 256.\"\n",
        "\n",
        "# Batch size\n",
        "batchSize = 4\n",
        "\n",
        "# Use motion blurs (data augmentation)\n",
        "# set True if training data contains images extracted from videos\n",
        "use_da_motion_blur = False \n",
        "\n",
        "# Use eye-aware training\n",
        "# require images generated from prep_binary_masks.ipynb\n",
        "use_bm_eyes = True\n",
        "\n",
        "# Probability of random color matching (data augmentation)\n",
        "prob_random_color_match = 0.5\n",
        "\n",
        "da_config = {\n",
        "    \"prob_random_color_match\": prob_random_color_match,\n",
        "    \"use_da_motion_blur\": use_da_motion_blur,\n",
        "    \"use_bm_eyes\": use_bm_eyes\n",
        "}\n",
        "\n",
        "# Path to training images\n",
        "img_dirA = './faceA/rgb'\n",
        "img_dirB = './faceB/rgb'\n",
        "img_dirA_bm_eyes = \"./binary_masks/faceA_eyes\"\n",
        "img_dirB_bm_eyes = \"./binary_masks/faceB_eyes\"\n",
        "\n",
        "# Path to saved model weights\n",
        "models_dir = \"./models\"\n",
        "\n",
        "# Architecture configuration\n",
        "arch_config = {}\n",
        "arch_config['IMAGE_SHAPE'] = (RESOLUTION, RESOLUTION, 3)\n",
        "arch_config['use_self_attn'] = True\n",
        "arch_config['norm'] = \"hybrid\" # instancenorm, batchnorm, layernorm, groupnorm, none\n",
        "arch_config['model_capacity'] = \"lite\" # standard, lite\n",
        "\n",
        "# Loss function weights configuration\n",
        "loss_weights = {}\n",
        "loss_weights['w_D'] = 0.1 # Discriminator\n",
        "loss_weights['w_recon'] = 1. # L1 reconstruction loss\n",
        "loss_weights['w_edge'] = 0.1 # edge loss\n",
        "loss_weights['w_eyes'] = 30. # reconstruction and edge loss on eyes area\n",
        "loss_weights['w_pl'] = (0.01, 0.1, 0.3, 0.1) # perceptual loss (0.003, 0.03, 0.3, 0.3)\n",
        "\n",
        "# Init. loss config.\n",
        "loss_config = {}\n",
        "loss_config[\"gan_training\"] = \"mixup_LSGAN\"\n",
        "loss_config['use_PL'] = False\n",
        "loss_config[\"PL_before_activ\"] = True\n",
        "loss_config['use_mask_hinge_loss'] = False\n",
        "loss_config['m_mask'] = 0.\n",
        "loss_config['lr_factor'] = 1.\n",
        "loss_config['use_cyclic_loss'] = False"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0620 10:52:20.633023 140576338810752 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bSrt2h0K3so3"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5o2dEVW92Vms",
        "colab": {}
      },
      "source": [
        "from networks.faceswap_gan_model import FaceswapGANModel\n",
        "from data_loader.data_loader import DataLoader\n",
        "from utils import showG, showG_mask, showG_eyes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yiNsc3N_2VhU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "7589bd5d-7887-49ac-ba3b-d4656394a32a"
      },
      "source": [
        "model = FaceswapGANModel(**arch_config)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0620 10:52:28.974577 140576338810752 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0620 10:52:28.996671 140576338810752 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0620 10:52:29.024400 140576338810752 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0620 10:52:29.578089 140576338810752 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCpSlBcwazyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!wget https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qvVr2TqI3jd9",
        "outputId": "3ee94aae-57d4-4e27-b19c-8bc8547350f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "#from keras_vggface.vggface import VGGFace\n",
        "\n",
        "# VGGFace ResNet50\n",
        "#vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))'\n",
        "\n",
        "from colab_demo.vggface_models import RESNET50\n",
        "vggface = RESNET50(include_top=False, weights=None, input_shape=(224, 224, 3))\n",
        "vggface.load_weights(\"rcmalli_vggface_tf_notop_resnet50.h5\")\n",
        "\n",
        "#from keras.applications.resnet50 import ResNet50\n",
        "#vggface = ResNet50(include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "#vggface.summary()\n",
        "\n",
        "model.build_pl_model(vggface_model=vggface, before_activ=loss_config[\"PL_before_activ\"])\n",
        "model.build_train_functions(loss_weights=loss_weights, **loss_config)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0620 10:52:36.541737 140576338810752 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0620 10:52:38.922918 140576338810752 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0620 10:52:38.982832 140576338810752 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0620 10:52:43.192040 140576338810752 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "W0620 10:52:46.394228 140576338810752 deprecation.py:323] From /content/faceswap-GAN/networks/losses.py:55: Beta.__init__ (from tensorflow.python.ops.distributions.beta) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "W0620 10:52:46.401965 140576338810752 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/beta.py:208: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "W0620 10:52:47.353548 140576338810752 deprecation_wrapper.py:119] From /content/faceswap-GAN/networks/losses.py:105: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GexJ-i7c3vz2"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yoCnWQiR3jgd",
        "colab": {}
      },
      "source": [
        "# Create ./models directory\n",
        "Path(f\"models\").mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bjkSygNT3jjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "58a75886-ba49-4345-b305-20bcfa39b2d6"
      },
      "source": [
        "# Get filenames\n",
        "train_A = glob.glob(img_dirA+\"/*.*\")\n",
        "train_B = glob.glob(img_dirB+\"/*.*\")\n",
        "\n",
        "train_AnB = train_A + train_B\n",
        "\n",
        "assert len(train_A), \"No image found in \" + str(img_dirA)\n",
        "assert len(train_B), \"No image found in \" + str(img_dirB)\n",
        "print (\"Number of images in folder A: \" + str(len(train_A)))\n",
        "print (\"Number of images in folder B: \" + str(len(train_B)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in folder A: 318\n",
            "Number of images in folder B: 376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-KzKahMU3jlX",
        "colab": {}
      },
      "source": [
        "def show_loss_config(loss_config):\n",
        "    for config, value in loss_config.items():\n",
        "        print(f\"{config} = {value}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j5fwrKth3jqO",
        "colab": {}
      },
      "source": [
        "def reset_session(save_path):\n",
        "    global model, vggface\n",
        "    global train_batchA, train_batchB\n",
        "    model.save_weights(path=save_path)\n",
        "    del model\n",
        "    del vggface\n",
        "    del train_batchA\n",
        "    del train_batchB\n",
        "    K.clear_session()\n",
        "    model = FaceswapGANModel(**arch_config)\n",
        "    model.load_weights(path=save_path)\n",
        "    #vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\n",
        "    vggface = RESNET50(include_top=False, weights=None, input_shape=(224, 224, 3))\n",
        "    vggface.load_weights(\"rcmalli_vggface_tf_notop_resnet50.h5\")\n",
        "    model.build_pl_model(vggface_model=vggface, before_activ=loss_config[\"PL_before_activ\"])\n",
        "    train_batchA = DataLoader(train_A, train_AnB, batchSize, img_dirA_bm_eyes,\n",
        "                              RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
        "    train_batchB = DataLoader(train_B, train_AnB, batchSize, img_dirB_bm_eyes, \n",
        "                              RESOLUTION, num_cpus, K.get_session(), **da_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ll7g7mGU3jss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "c4ac5f6c-d127-4f50-e277-8f837b99b5c5"
      },
      "source": [
        "# Start training\n",
        "t0 = time.time()\n",
        "\n",
        "# This try/except is meant to resume training if we disconnected from Colab\n",
        "try:\n",
        "    gen_iterations\n",
        "    print(f\"Resume training from iter {gen_iterations}.\")\n",
        "except:\n",
        "    gen_iterations = 0\n",
        "\n",
        "errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
        "errGAs = {}\n",
        "errGBs = {}\n",
        "# Dictionaries are ordered in Python 3.6\n",
        "for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
        "    errGAs[k] = 0\n",
        "    errGBs[k] = 0\n",
        "\n",
        "display_iters = 300\n",
        "global TOTAL_ITERS\n",
        "\n",
        "global train_batchA, train_batchB\n",
        "train_batchA = DataLoader(train_A, train_AnB, batchSize, img_dirA_bm_eyes, \n",
        "                          RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
        "train_batchB = DataLoader(train_B, train_AnB, batchSize, img_dirB_bm_eyes, \n",
        "                          RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
        "\n",
        "while gen_iterations <= TOTAL_ITERS: \n",
        "    \n",
        "    # Loss function automation\n",
        "    if gen_iterations == (TOTAL_ITERS//5 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = False\n",
        "        loss_config['m_mask'] = 0.0\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (TOTAL_ITERS//5 + TOTAL_ITERS//10 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.5\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Complete.\")\n",
        "    elif gen_iterations == (2*TOTAL_ITERS//5 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.2\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (TOTAL_ITERS//2 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.4\n",
        "        loss_config['lr_factor'] = 0.3\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (2*TOTAL_ITERS//3 - display_iters//2):\n",
        "        clear_output()\n",
        "        model.decoder_A.load_weights(\"models/decoder_B.h5\") # swap decoders\n",
        "        model.decoder_B.load_weights(\"models/decoder_A.h5\") # swap decoders\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.5\n",
        "        loss_config['lr_factor'] = 1\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        for x in os.listdir('./models/'):\n",
        "            files.download(os.path.join('./models/'+ x))\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (8*TOTAL_ITERS//10 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.1\n",
        "        loss_config['lr_factor'] = 0.3\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        for x in os.listdir('./models/'):\n",
        "            files.download(os.path.join('./models/'+ x))        \n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (9*TOTAL_ITERS//10 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = False\n",
        "        loss_config['m_mask'] = 0.\n",
        "        loss_config['lr_factor'] = 0.5\n",
        "        loss_config['use_cyclic_loss'] = True\n",
        "        loss_weights['w_D'] = 0. # Discriminator\n",
        "        loss_weights['w_recon'] =0. # L1 reconstruction loss\n",
        "        loss_weights['w_edge'] = 0. # edge loss\n",
        "        loss_weights['w_eyes'] = 0. # reconstruction and edge loss on eyes area\n",
        "        loss_config['use_PL'] = False\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        for x in os.listdir('./models/'):\n",
        "            files.download(os.path.join('./models/'+ x))\n",
        "        print(\"Done.\")\n",
        "    \n",
        "    if gen_iterations == 5:\n",
        "        print (\"working.\")\n",
        "    \n",
        "    # Train dicriminators for one batch\n",
        "    data_A = train_batchA.get_next_batch()\n",
        "    data_B = train_batchB.get_next_batch()\n",
        "    errDA, errDB = model.train_one_batch_D(data_A=data_A, data_B=data_B)\n",
        "    errDA_sum +=errDA[0]\n",
        "    errDB_sum +=errDB[0]\n",
        "\n",
        "    # Train generators for one batch\n",
        "    data_A = train_batchA.get_next_batch()\n",
        "    data_B = train_batchB.get_next_batch()\n",
        "    errGA, errGB = model.train_one_batch_G(data_A=data_A, data_B=data_B)\n",
        "    errGA_sum += errGA[0]\n",
        "    errGB_sum += errGB[0]\n",
        "    for i, k in enumerate(['ttl', 'adv', 'recon', 'edge', 'pl']):\n",
        "        errGAs[k] += errGA[i]\n",
        "        errGBs[k] += errGB[i]\n",
        "    gen_iterations+=1\n",
        "    \n",
        "    # Visualization\n",
        "    if gen_iterations % display_iters == 0:\n",
        "        clear_output()\n",
        "            \n",
        "        # Display loss information\n",
        "        show_loss_config(loss_config)\n",
        "        print(\"----------\") \n",
        "        print('[iter %d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f time: %f'\n",
        "        % (gen_iterations, errDA_sum/display_iters, errDB_sum/display_iters,\n",
        "           errGA_sum/display_iters, errGB_sum/display_iters, time.time()-t0))  \n",
        "        print(\"----------\") \n",
        "        print(\"Generator loss details:\")\n",
        "        print(f'[Adversarial loss]')  \n",
        "        print(f'GA: {errGAs[\"adv\"]/display_iters:.4f} GB: {errGBs[\"adv\"]/display_iters:.4f}')\n",
        "        print(f'[Reconstruction loss]')\n",
        "        print(f'GA: {errGAs[\"recon\"]/display_iters:.4f} GB: {errGBs[\"recon\"]/display_iters:.4f}')\n",
        "        print(f'[Edge loss]')\n",
        "        print(f'GA: {errGAs[\"edge\"]/display_iters:.4f} GB: {errGBs[\"edge\"]/display_iters:.4f}')\n",
        "        if loss_config['use_PL'] == True:\n",
        "            print(f'[Perceptual loss]')\n",
        "            try:\n",
        "                print(f'GA: {errGAs[\"pl\"][0]/display_iters:.4f} GB: {errGBs[\"pl\"][0]/display_iters:.4f}')\n",
        "            except:\n",
        "                print(f'GA: {errGAs[\"pl\"]/display_iters:.4f} GB: {errGBs[\"pl\"]/display_iters:.4f}')\n",
        "        \n",
        "        # Display images\n",
        "        print(\"----------\") \n",
        "        wA, tA, _ = train_batchA.get_next_batch()\n",
        "        wB, tB, _ = train_batchB.get_next_batch()\n",
        "        print(\"Transformed (masked) results:\")\n",
        "        showG(tA, tB, model.path_A, model.path_B, batchSize)   \n",
        "        print(\"Masks:\")\n",
        "        showG_mask(tA, tB, model.path_mask_A, model.path_mask_B, batchSize)  \n",
        "        print(\"Reconstruction results:\")\n",
        "        showG(wA, wB, model.path_bgr_A, model.path_bgr_B, batchSize)           \n",
        "        errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
        "        for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
        "            errGAs[k] = 0\n",
        "            errGBs[k] = 0\n",
        "        \n",
        "        # Save models\n",
        "        model.save_weights(path=models_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0620 11:07:08.353250 140576338810752 deprecation.py:323] From /content/faceswap-GAN/data_loader/data_loader.py:58: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0620 11:07:08.354527 140576338810752 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0620 11:07:08.359458 140576338810752 deprecation.py:323] From /content/faceswap-GAN/data_loader/data_loader.py:54: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0620 11:07:08.392448 140576338810752 deprecation.py:323] From /content/faceswap-GAN/data_loader/data_loader.py:64: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "working.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kRD_02DL3ju9",
        "colab": {}
      },
      "source": [
        "import os \n",
        "for x in os.listdir('./models/'):\n",
        "    files.download(os.path.join('./models/'+ x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lSqr_UTX4wRE"
      },
      "source": [
        "## The following cells are from [FaceSwap_GAN_v2.2_video_conversion.ipynb](https://github.com/shaoanlu/faceswap-GAN/blob/master/FaceSwap_GAN_v2.2_video_conversion.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x58lrjkw6iQM"
      },
      "source": [
        "## Video conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EOi2ZsSa4vf4",
        "colab": {}
      },
      "source": [
        "from converter.video_converter import VideoConverter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmQ7W0zsaz2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global model, vggface\n",
        "global train_batchA, train_batchB\n",
        "del model\n",
        "del vggface\n",
        "del train_batchA\n",
        "del train_batchB\n",
        "tf.reset_default_graph()\n",
        "K.clear_session()\n",
        "model = FaceswapGANModel(**arch_config)\n",
        "model.load_weights(path=models_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R4uUub3A4vi7",
        "colab": {}
      },
      "source": [
        "fd = MTCNNFaceDetector(sess=K.get_session(), model_path=\"./mtcnn_weights/\")\n",
        "vc = VideoConverter()\n",
        "vc.set_face_detector(fd)\n",
        "vc.set_gan_model(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MxwjrVoc4vmQ",
        "colab": {}
      },
      "source": [
        "options = {\n",
        "    # ===== Fixed =====\n",
        "    \"use_smoothed_bbox\": True,\n",
        "    \"use_kalman_filter\": True,\n",
        "    \"use_auto_downscaling\": False,\n",
        "    \"bbox_moving_avg_coef\": 0.65,\n",
        "    \"min_face_area\": 35 * 35,\n",
        "    \"IMAGE_SHAPE\": model.IMAGE_SHAPE,\n",
        "    # ===== Tunable =====\n",
        "    \"kf_noise_coef\": 1e-3,\n",
        "    \"use_color_correction\": \"hist_match\",\n",
        "    \"detec_threshold\": 0.8,\n",
        "    \"roi_coverage\": 0.9,\n",
        "    \"enhance\": 0.,\n",
        "    \"output_type\": 3,\n",
        "    \"direction\": \"AtoB\", # ==================== This line determines the transform direction ====================\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qyuELtb94vpr",
        "colab": {}
      },
      "source": [
        "if options[\"direction\"] == \"AtoB\":\n",
        "    input_fn = fn_source_video\n",
        "    output_fn = \"OUTPUT_VIDEO_AtoB.mp4\"\n",
        "elif options[\"direction\"] == \"BtoA\":\n",
        "    input_fn = fn_target_video\n",
        "    output_fn = \"OUTPUT_VIDEO_BtoA.mp4\"\n",
        "\n",
        "duration = None # None or a non-negative float tuple: (start_sec, end_sec). Duration of input video to be converted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sem7RMzm4vr6",
        "colab": {}
      },
      "source": [
        "vc.convert(input_fn=input_fn, output_fn=output_fn, options=options, duration=duration)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QwKOc5Jt5oFO"
      },
      "source": [
        "# Download result video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M0MkktlHC5sh",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hwDUxwlL4voW",
        "colab": {}
      },
      "source": [
        "if options[\"direction\"] == \"AtoB\":\n",
        "    files.download(\"OUTPUT_VIDEO_AtoB.mp4\")\n",
        "elif options[\"direction\"] == \"BtoA\":\n",
        "    files.download(\"OUTPUT_VIDEO_BtoA.mp4\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
